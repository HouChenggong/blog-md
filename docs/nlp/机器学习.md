## 智能问答与深度学习

[学习地址](https://gitee.com/chatopera/book-of-qna-code)

### 基础

#### [马尔可夫链](https://www.zhihu.com/question/26665048)

- 随机过程

  比如今天下雨、明天下雨、后天下雨的概率，从今天下雨到明天下雨，再到后天下雨其实就是一个过程，一个可以用公式推导的随机过程。比如预测天气、股票等

- 马尔可夫链

就是状态的转移是有概率的，比如

 

|      | 吃   | 睡   | 玩   |
| ---- | ---- | ---- | ---- |
| 吃   | 0.2  | 0.3  | 0.5  |
| 睡   | 0.1  | 0.6  | 0.3  |
| 玩   | 0.4  | 0.5  | 0.1  |

这个矩阵就是**转移概率矩阵P**，并且它是保持不变的，就是说第一天到第二天的转移概率矩阵跟第二天到第三天的转移概率矩阵是一样的

 总结：马尔可夫链就是这样一个任性的过程，它将来的状态分布只取决于现在，跟过去无关！

马尔科夫假设这里相当于就是个2-gram。

#### [隐马尔可夫模型——HMM](https://www.zhihu.com/question/20962240/answer/64187492)

举个例子，我女朋友现在在北京工作，而我还在法国读书。每天下班之后，她会根据天气情况有相应的活动：或是去商场购物，或是去公园散步，或是回家收拾房间。我们有时候会通电话，她会告诉我她这几天做了什么，而闲着没事的我呢，则要通过她的行为猜测这几天对应的天气最有可能是什么样子的。

HMM要解决三个问题：

- 已知模型，告诉结果，求这个结果的概率

  比如：女朋友告诉我这三天的行为，求这三个行为的概率

  T天N种行为,求每种概率的算法复杂度是：N^T，即N的T次方

  - 算法：暴力求解（当行为和天数非常多的时候）复杂度是：N^T
  - 向前算法：每次都是在前一次的基础上进行的，需要累加T次，每次的复杂度是N^2,最后的复杂度是T*N^2
  - [向后算法Baum-Welch](https://zhuanlan.zhihu.com/p/41912745)
  - 解决方案：
    - 穷举每种可能算概率
    - 罗列每一步的概率，下一次跟前一次有关

- 已知模型，告诉结果，求隐形相关结果

  比如：女朋友告诉我这三天的行为，让我猜这三天的天气

  - [维特比算法](https://www.zhihu.com/question/20136144)：维特比算法致力于寻找一条最佳路径，以便能最好地解释观测到的序列。

  - 维特比算法多用于词性标注

    算法的核心思想是：通过找多条路径到达同一条的最优解，进而删除其它不是最优解的路径

- 求模型？给结果求模型

  - 鲍姆-韦尔奇算法





### Tree树

用于前缀匹配

#### BinTrie

Tree树是用Map存储的，而我们知道map存储有优点也有缺点，当存在大量数据的时候，map会不断的扩容，而且查询key的代价还是有点大的，所以map在词典的场景中不是很试用

而BinTrie直接new 了一个65536（ (int)Character.MAX_VALUE+1）的数组空间，因为java 中char的最大值就是2^ 16，所以这里是肯定够用的，直接获取首字母的char，存入进去，每次访问的时候只需要访问下标即可，速度极快

首个字母：用char(字符)确定位置,其余的肯定不能再每个都申请一个65536的数组，而是本来是0，慢慢一个个的用二分查找法插入

### AC自动机

AC自动机能高速完成多模式匹配，然而具体实现聪明与否决定最终性能高低。大部分实现都是一个Map<Character, State>了事，无论是TreeMap的对数复杂度，还是HashMap的巨额空间复杂度与哈希函数的性能消耗，都会降低整体性能。



算法讲解：https://www.bilibili.com/video/BV1uJ411Y7Eg?p=4

 





### [双数组Tree树](https://www.zhihu.com/question/352900751)

双数组Trie树能高速O(n)完成单串匹配，并且内存消耗可控，然而软肋在于多模式匹配，如果要匹配多个模式串，必须先实现前缀查询，然后频繁截取文本后缀才可多匹配，这样一份文本要回退扫描多遍，性能极低。

构建时间比较长，一旦有新词加入，搞不好要全树进行重构。比如一个词的首字母没有出现过，现在写入若出现冲突，就需要改写根节点的转移基数，那么之前构建好的词都需要重构。



建议：首先构建首字，然后追一构建各个节点的子节点，一旦产生冲突，可以将冲突的处理局限在父子节点之间，不至于产生大规模的重构

这两个绝对能看懂

https://blog.csdn.net/kissmile/article/details/47417277

https://segmentfault.com/a/1190000008877595

https://blog.csdn.net/i_love_zxy/article/details/103547251

```
protected int check[];
protected int base[];
   protected V[] v;//存储的词典
```

具体来说就是使用两个数组base[]和check[]来维护Trie树，base[]负责记录状态，check[]用于检验状态转移的正确性，当check[i]为负值时，表示此状态为字符串的结束。



具体来说，当状态b接受字符c然后转移到状态p的时候，满足的状态转移公式如下：

```
 
base[0] = 1 
check[0] = 0 
p = base[b] + c
check[p] = base[c]	


1 建立根节点root,令base[root] =1
2 找出root的子节点 集{root.childreni }(i = 1...n) , 使得 check[root.childreni ] = base[root] = 1
3 对 each element in  root.children : 
　　1）找到{elemenet.childreni }(i = 1...n) ，注意若一个字符位于字符序列的结尾，则其孩子节点包括一个空节点，其code值设置为0找到一个值begin使得每一个check[ begini + element.childreni .code] = 0
　　2）设置base[element.childreni] = begini
　　3）对element.childreni 递归执行步骤3，若遍历到某个element，其没有children，即叶节点，则设置base[element]为负值（一般为在字典中的index取负）

```

```
char =      ×    中    华    华     ×    人     ×    华     ×     ×     ×    学    新    清    大
charCode =  × 20013 21326 21326  × 20154  × 21326  ×  ×  × 23398 26032 28165 22823
base =      1     2  1176 21330    -1 21332    -2 21334    -3    -4    -5 23400 23401     6     3
i    =      0 20015 21328 21329 21330 21331 21332 21333 21334 23400 23401 23402 26039 28167 44158
check=      0     1     1     2 21330  1176 21332     6 21334 23400 23401     3     6     1 21334
```

- base基数是怎么来的？

即上面的结果也验证了一个词在base数组的index=base[字符]+code(字符)

但是code（字符）我们知道就是字符的ascii编码，但是base（字符）是怎么来的呢？

我们从插入过程开始说起：

一个词在base数组的位置：base[字符]+code（字符）=0+code(字符)=X ，假如这个X已经存在了，那么将base(字符)+1，如果一直冲突，则继续+1，直到找到一个不冲突的位置，并记录base（code）

- 为啥会有多个位置呢？

因为华可能是首字母，有可能是别的词的中间值，而我们计算一个词在base数组中的位置是通过前一个词的转移基数加上来的

比如base《清》=6，那么base《清华》=base《清》+code(华)=

- 怎么表示清华、清华大学

在清华的华上面，直接将base基础改成负数

- 有了base为啥还要有check数组？

  因为要判断一个词在数组中是否存在，用的是base(字符)+code(字符)，如果这个位置在，说明是存在的，单其实不一定存在

  而check数组是存储它上级节点的位置，如果p = base[b] + c。check[p] = base[c]则代表存在，否则不存在	
  
- 为啥要用tree树来构建双数组Tree树？

因为双数组tree树首先要确定首字符，来避免冲突，tree树完美解决了这个问题

AC和双数组的对比

https://www.hankcs.com/program/algorithm/double-array-trie-vs-aho-corasick-double-array-trie.html



怎么支持多模式匹配：写了一个Searcher结构，里面储存了当前扫描的起点，并且用一个链表储存了从当前起点开始途经的所有词串。接着只要不断地将起点往后挪一个，就支持了多模式匹配。也就是这个“挪一个单位”的动作，让我认为DAT在多模式匹配上，复杂度更高（应该是O(n2)，n是母文本的长度）。要知道，理论上AC自动机是线性复杂度的。

### 双数组AC自动机

构建过程：

1、构建tree树

2、利用tree树构建双数组tree树

3、构建AC自动机



核心词典都保存在CoreDictionary中，自定义词典保存在CustomDictionary中，用户自定义的词保存在BinTrie中

当用户新增自定义词典的时候，会向CustomDist中查询是否存在，如果存在则替换CustomDictionary中的值（但是文件和缓存是不会进行更新的），如果不存在插入到BinTrie中

代码逻辑在：CustomDictionary.insert()方法中可以看到

执行搜索的流程是什么呢？



```
DoubleArrayTrie中的dat并不包含核心词典中的值，也不包含用户动态插入的值，只是保存在自定义词典文件中的值
```

搜索流程：去WordBasedSegment的generateWordNet的方法，去核心词典搜索，获取所有可能性，得到一个粗分的词网

```
 // 核心词典查询
        DoubleArrayTrie<CoreDictionary.Attribute>.Searcher searcher = CoreDictionary.trie.getSearcher(charArray, 0);
```

然后调用维特比算法得到一个最段路

如果开启了自定义词典，执行下面的，注意会传入一个dat，但是这个dat并不少核心词典，而是CustomDictionary的dat

```
combineByCustomDictionary(vertexList, this.dat);

    public ViterbiSegment()
    {
    // 自定义词典的dat
        this.dat = CustomDictionary.dat;
    }
```

先进行合并用户自定义的词典，原理是：通过双数组判断，下一个词和本词是否能取得联系，用的就是双数组的判断，如果能取得联系，则合并，代码在：

```
Segement的combineByCustomDictionary方法中
```

上面的合并了用户自定义的词典，单如果是自己加载进去的词典呢？就是那些保存在BinTrie中的词，如何合并的呢？

其实就是利用tree树的特性进行快速查询

https://www.cnblogs.com/hapjin/p/11172299.html



 

缺点：当词汇不长时，这使得AC自动机的fail机制没有用武之地

即：当有短模式串的时候，优先使用DAT，否则优先使用ACDAT





#### 自定义词性

```
LexiconUtility.setAttribute("苹果电脑", "电脑品牌 1000");

    public static boolean setAttribute(String word, CoreDictionary.Attribute attribute)
    {
        if (attribute == null) return false;

        if (CoreDictionary.trie.set(word, attribute)) return true;
        if (CustomDictionary.dat.set(word, attribute)) return true;
        if (CustomDictionary.trie == null)
        {
            CustomDictionary.add(word);
        }
        CustomDictionary.trie.put(word, attribute);
        return true;
    }
```

```
  // 直接插入用户词典
  public static boolean insert(String word, String natureWithFrequency)
    {
        if (word == null) return false;
        if (HanLP.Config.Normalization) word = CharTable.convert(word);
        CoreDictionary.Attribute att = natureWithFrequency == null ? new CoreDictionary.Attribute(Nature.nz, 1) : CoreDictionary.Attribute.create(natureWithFrequency);
        if (att == null) return false;
        if (dat.set(word, att)) return true;
        if (trie == null) trie = new BinTrie<CoreDictionary.Attribute>();
        trie.put(word, att);
        return true;
    }
```

```
DemoPipeline 正则处理

hanlp语义相似度
海明距离


或者 "minimum_should_match":75%，可以配置一个一个百分比，至少optional clauses至少满足75%，这里是向下取整的。
比如有5个clause,5*75%=3.75,向下取整为3，也就是至少需要match 3个clause。
//minimum_should_match将的比较好
https://blog.csdn.net/qq_22985751/article/details/90704189



有一个想法，就是搜索的时候，填写哪些是必须要搜索到的，哪些可以不搜索到，百度和谷歌都有这个功能，其实就是让这个词不被分词
那么就有这个功能了：将不被分词的提到最前面，分过词的放到后面

API回去好好看下
https://www.elastic.co/guide/en/elasticsearch/reference/6.4/mixing-exact-search-with-stemming.html

搜索的问题，有些回答不错
https://elasticsearch.cn/question/11084


rescore 重新打分数 function_score这个比较好

https://blog.csdn.net/lijingjingchn/article/details/106408774
https://blog.csdn.net/wwd0501/article/details/78652850

介绍高斯的
https://blog.csdn.net/weixin_40341116/article/details/81003513
 "gauss": {
        "location": {
          "origin": { "lat": 40, "lon": 116 },
          "offset": "5km",
          "scale":  "10km"
           }
         }

```

IK 分词原理

https://www.jianshu.com/p/979f4ba31512

http://3dobe.com/archives/44/





http://wiki.vipkid.com.cn/pages/viewpage.action?pageId=175530591



1、正则匹配

2、如果数量级比较少，我们把每一句话当作一个词来处理，然后看分出的词，其实就是一个模版

3、如果较多的情况下，需要识别词性以及分词


### 组合分词
#### 最大匹配算法

大、学生、大学生、活动、中心

搜索：大学生活动中心

- 正向匹配：最大匹配出的词必须保证下一个扫描不是词表中的词或词的前缀才可以结
  - 先搜索：大学生活动中心、大学生活动中、大学生活动、大学生活、**大学生**
  - 再搜索：活动中心、活动中、**活动**
  - 再搜索：**中心**
- 逆向匹配：
  - 先搜索：大学生活动中心、学生活动中心、生活动中心、活动中心、动中心、**中心**
  - 再搜索：大学生活动、学生活动、生活动、**活动**
  - 再搜索:  **大学生**

​      

-  双向最大匹配法：FMM和BMM两种算法都分词一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。 

如：“我们在野生动物园玩”

正向最大匹配法，最终分词结果为：“我们/在野/生动/物/园/玩”，其中，总分词数6个，单个词为3。

逆向最大匹配法，最终分词结果为：“我们/在/野生动物园/玩”，其中，总分词数4个，单个词为2。

选择标准：

1. 同时执行正向和逆向最长匹配
2. 若两者的词数不同，则返回次数更少的哪一个
3. 若两者的词数相同，则返回两者中单字更少的哪一个
4. 若两者的单字数也相同，则优先返回逆向最长匹配的结果

因此最终输出为逆向结果。

上面的都没有引入词和词之间的概率问题，如果有概率问题怎么办？

 #### 基于概率进行分词组合

在一个语料库中假设有1万个词，每个词出现的次数不是固定的，也就是有词频，词频组成了一元词典，即词和词频，有了词频就有了概率，即中国出现了100次，概率=100/10000=0.01，有了概率就可以根据概率进行组合分词了，其实就是求最大概率的结果

- 暴力求解
- 向前、向后算法
- 维特比算法

#### 基于N-gram分词

即二元词典比如：中国@人 

三元词典：小明@爱@爬山

N的影响：N越大，精度越高、概率越小、词典越大，超过3个的一般认为没有意义

比如有2000个词，N=N，有2000^ N



 

TF词频

(freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength))

IDF逆文档频率

log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) 



docFreq：当前分片包含关键词的文档个数
docCount：当前分片的个数

termFreq：当前文档分词后的terms匹配关键词的个数
k1、b 是BM25算法的两个调节因子，文章开头已经简述
avgFieldLength：当前分片的平均term（当前分片所有文档的分词个数/当前分片总文档数）
fieldLength：本文档的分词数



TF词频越高——得分越高

IDF逆词频越高——得分越低

字段越短——得分越高



BM25，对词频进行改进，TF越高，整个曲线会趋向一个数值



```
DemoUseAhoCorasickDoubleArrayTrieSegment
运行用户直接使用自己的词典分词

DemoOccurrence获取NGram模型
```





HMM词性标注

https://www.jianshu.com/p/5a2455bd0abf



Java实现词性标注：https://blog.csdn.net/Charzous/article/details/109138830



比如：吃 水果   chi shuiguo 2个词，但是用户自定义了一个吃水果，那么它的词性应该是：把前面一个也要加上

NGram模型无法解决新词的问题，它无法识别

**HMM是：解决词性标注和命名体识别的重要手段**

比如根据天气求活动，根据表现求病情







City weather

City1 weather

City2 weather

City——ci ty1 city2

用正则表达式匹配，先分词，然后找出词槽的那个词，如下：

1. `.{0,3}${date}${city}[的]天气[预报|情况|状况].{0,3}`
2. 但是也会有一个问题，比如北京属于chengshi和city，这样它们又进行组合了，很头痛











用户词典：用户词典一般用于长词，越长越好，比如：专业术语、成语、习语

对于较短，容易引起歧义的词语，需要通过标注语料库、训练模型的方式来解决



HMM 分词

B MES-中文分词

为了捕获汉字分别作为BMES时不同的概率，人名提出了{B,M,E,S}这种最流行的标注集

标注后分词器将最近两个BE标签对应区间内所有的字符合并为一个词语，S对应标签作为单字词语，按照顺序输出完成分词过程

BMES中文序列标注

简单的人名和地名可以通过中文分词切分，然后通过词性标注来确定所属类别，但是地名和机构名往往是多个单词组成（复合词），所以命名体识别常常在分词和词性标注结果之后再进行识别召回。

StatusSet: 状态值集合——状态值集合为(B, M, E, S):
ObservedSet: 观察值集合，语料库集合
TransProbMatrix: 转移概率矩阵
EmitProbMatrix: 发射概率矩阵
InitStatus: 初始状态分布

小明硕士毕业于中国科学院计算所
输出的状态序列为

BEBEBMEBEBMEBES
根据这个状态序列我们可以进行切词:

BE/BE/BME/BE/BME/BE/S
所以切词结果如下:

小明/硕士/毕业于/中国/科学院/计算/所

一阶HMM分词不依赖词典，而是由一个模型直接训练，得到词和词之间关于BMES之间的转移函数（每个状态仅仅依赖前面一个状态），效果很差，

HMM-Bi-Gram



不如词典分词，利用二阶HMM（每个状态依赖前面2个状态）进行分词呢？

HMM-TriGram

https://www.hankcs.com/nlp/segment/second-order-hidden-markov-model-trigram-chinese-participle.html

但是命名体识别依然可以沿用BMES标注集，并沿用中文分词的逻辑，只不过把标注的对象变为单词而已，唯一不同的是命名体识别还需要确定实体的类别，比如构成地名的单词标注为：BMES-地名，对于那些不构成命名体识别的单词，统一标注为O（outSide）

比如《参观（O）了（O） 北京（B-地名）天安门（E-地名）》

​	商品和服务

B- begin

E-End

M-Middle词中

S-Single单字成词







KMP 算法https://www.zhihu.com/question/21923021



http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html

