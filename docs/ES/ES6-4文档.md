## ES

一个不错的网址：https://www.cnblogs.com/Neeo/articles/10695019.html

### ES集群和单个节点

群集是一个或多个节点（服务器）的集合，这些节点一起保存您的全部数据，并在所有节点之间提供联合索引和搜索功能。集群由唯一名称标识，默认情况下为“ elasticsearch“

节点是单个服务器，它是群集的一部分，存储数据并参与群集的索引和搜索功能。就像群集一样，节点由名称标识，该名称默认为在启动时分配给该节点的随机通用唯一标识符（UUID）

### 搜索API

对于响应，我们看到以下部分：

- `took` – Elasticsearch执行搜索的时间（以毫秒为单位）
- `timed_out` –告诉我们搜索是否超时
- `_shards` –告诉我们搜索了多少个分片，以及成功/失败的搜索分片的数量
- `hits` - 搜索结果
- `hits.total` –符合我们搜索条件的文档总数
- `hits.hits` –搜索结果的实际数组（默认为前10个文档）
- `hits.sort` -结果排序键（如果按得分排序则丢失）
- `hits._score`并且`max_score`-现在暂时忽略这些字段

```json
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1000,
    "max_score" : null,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "_doc",
      "_id" : "0",
      "sort": [0],
      "_score" : null,
      "_source" : {"account_number":0,"balance":16623,"firstname":"Bradshaw","lastname":"Mckenzie","age":29,"gender":"F","address":"244 Columbus Place","employer":"Euron","email":"bradshawmckenzie@euron.com","city":"Hobucken","state":"CO"}
    }, {
      "_index" : "bank",
      "_type" : "_doc",
      "_id" : "1",
      "sort": [1],
      "_score" : null,
      "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, ...
    ]
  }
}
```

### suggest

Term

term 词项建议器，对给入的文本进行分词，为每个词进行模糊查询提供词项建议。对于在索引中存在词默认不提供建议词，不存在的词则根据模糊查询结果进行排序后取一定数量的建议词。

 phrase

phrase 短语建议，在term的基础上，会考量多个term之间的关系，比如是否同时出现在索引的原文里，相邻程度，以及词频等

 completion

自动补全前缀形式的

**suggest的三个核心功能**

1、匹配：能够通过用户的输入进行前缀匹配；

2、排序：根据建议词的优先级或者搜索热度进行排序；

3、纠错：能够对用户的输入进行拼写纠错（suggest建议优先prefix匹配，不宜过多提示，因此只需提供前缀匹配，中文拼音匹配即可

所以适用场景多是：商品搜索的前缀搜索，比如搜一个卫衣，会出现一大堆，卫衣男、卫衣女、卫衣加绒，但是它不会出现男卫衣、女卫衣，所以都是用于前缀匹配的



```java
SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
SuggestionBuilder termSuggestionBuilder =
    SuggestBuilders.termSuggestion("user").text("kmichy"); 
SuggestBuilder suggestBuilder = new SuggestBuilder();
suggestBuilder.addSuggestion("suggest_user", termSuggestionBuilder); 
searchSourceBuilder.suggest(suggestBuilder);


//TermSuggestionBuilder为user字段和文本创建一个新的kmichy


//添加建议生成器并命名 suggest_user
```



从Response中获取搜索建议

```java
Suggest suggest = searchResponse.getSuggest(); 
TermSuggestion termSuggestion = suggest.getSuggestion("suggest_user"); 
for (TermSuggestion.Entry entry : termSuggestion.getEntries()) { 
    for (TermSuggestion.Entry.Option option : entry) { 
        String suggestText = option.getText().string();
    }
}
```





```java
 
 public JSONObject getSearchSuggest(String key) {
   //最多保留20个字符
   
     CompletionSuggestionBuilder suggestion = SuggestBuilders
             .completionSuggestion("标准问题字段").prefix(key).size(20).skipDuplicates(true);
     SuggestBuilder suggestBuilder = new SuggestBuilder();
   //这个是索引的type
     suggestBuilder.addSuggestion("knowledge的type", suggestion);
     SearchResponse response = template.suggest(suggestBuilder, EsConstants.SUGGEST);
     Suggest suggest = response.getSuggest();
 
     Set<String> keywords = null;
     if (suggest != null) {
         keywords = new HashSet<>();
         List<? extends Suggest.Suggestion.Entry<? extends Suggest.Suggestion.Entry.Option>> entries = suggest.getSuggestion("suggest").getEntries();
 
         for (Suggest.Suggestion.Entry<? extends Suggest.Suggestion.Entry.Option> entry: entries) {
             for (Suggest.Suggestion.Entry.Option option: entry.getOptions()) {
                 /** 最多返回9个推荐，每个长度最大为20 */
                 String keyword = option.getText().string();
                 if (!StringUtils.isEmpty(keyword) && keyword.length() <= 10) {
                     /** 去除输入字段 */
                     if (keyword.equals(key)) continue;
                     keywords.add(keyword);
                     if (keywords.size() >= 9) {
                         break;
                     }
                 }
             }
         }
     }
     return ApiResult.OK(keywords, "获取推荐词组成功");
 }        
 
```







# ElaticSearch

- 操作ES的restful语法

```java
GET请求：
	http://ip:port/index :查询索引信息
	http://ip:port/index/type/doc_id :查询指定的文档信息
POST请求：
    http://ip:port/index/type/_search: 查询文档，可以在请求体中添加json字符串来代表查询条件
    http://ip:port/index/type/doc_id/_update: 修改文档，在请求体中添加json字符串来代表修改的信息
PUT请求：
    http://ip:port/index : 创建一个索引，需要在请求体中指定索引的信息
    http://ip:port/index/type/_mappings:代表创建索引时，指定索引文档存储属性的信息
DELETE 请求：
    http://ip:port/index： 删除跑路
    http://ip:port/index/type/doc_id:  删除指定的文档
```

```java
字符串类型:
  text: 一般用于全文检索，将当前field 进行分词
  keyword:当前field  不会进行分词
```



```sql
#创建索引，指定数据类型
PUT /book
{
  "settings": {
    #分片数
    "number_of_shards": 5,
    #备份数
    "number_of_replicas": 1
  },
    #指定数据类型
 "mappings": {
    #类型 Type
   "novel":{
    #文档存储的field
     "properties":{
       #field属性名
       "name":{
         #类型
         "type":"text",
         #指定分词器
         "analyzer":"ik_max_word",
         #指定当前的field可以被作为查询的条件
         "index":true,
         #是否需要额外存储
         "store":false
       },
       "author":{
         "type":"keyword"
       },
       "count":{
         "type":"long"
       },
       "on-sale":{
         "type":"date",
           #指定时间类型的格式化方式
         "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
       },
        "descr":{
          "type":"text",
          "analyzer":"ik_max_word"
       }
     }
   }
 }
}
```

### ES 集群健康

集群健康状态颜色分为 绿色,黄色或红色.

- Green - everything is good (cluster is fully functional). 绿色 - 所有功能健康(集群功能齐全);
- Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional). 黄色 - 所有数据都可用,但尚未分配一些副本(群集功能齐全);
- Red - some data is not available for whatever reason(cluster is partially functional). 红色 - 某些数据由于某种原因不可用(群集部分功能).

```java

//要检查群集运行状况,我们可以使用_cat命令.
GET /_cat/health?v
//检查节点健康状态
GET /_cat/nodes?v
//查看集群索引状态
GET /_cat/indices?v
// 检查节点分配信息
//GET _cat/allocation?v  

```

### ES Kibana操作



```java
// 查询一个索引xiyou_student_index的结构
GET /xiyou_student_index/_mapping

//查询一个具体的词在ES中的分词效果

POST _analyze
{"analyzer": "ik_smart","text": "测试一个app两个群"}


// 将java的sourceBuilder打印出来直接拷贝到Kibana中查询
GET /xxx索引/_search
{
    "query":{
        "bool":{
            "must":[
                {
                    "match":{
                        "xxxId":{
                            "query":45,
                            "operator":"OR",
                            "prefix_length":0,
                            "max_expansions":50,
                            "fuzzy_transpositions":true,
                            "lenient":false,
                            "zero_terms_query":"NONE",
                            "auto_generate_synonyms_phrase_query":true,
                            "boost":1
                        }
                    }
                },
                {
                    "multi_match":{
                        "query":"两个群",
                        "fields":[
                            "字段1^1.0",
                            "字段2^0.5",
                            "字段3^1.5"
                        ],
                        "type":"best_fields",
                        "operator":"OR",
                        "slop":0,
                        "prefix_length":0,
                        "max_expansions":50,
                        "zero_terms_query":"NONE",
                        "auto_generate_synonyms_phrase_query":true,
                        "fuzzy_transpositions":true,
                        "boost":1
                    }
                }
            ],
            "adjust_pure_negative":true,
            "boost":1
        }
    },
    "highlight":{
        "pre_tags":[
            "<font color='#FF7043'>"
        ],
        "post_tags":[
            "</font>"
        ],
        "fields":{
            "字段1":{

            },
            "字段2":{

            },
            "字段3":{

            }
        }
    }
}
```



## 1.索引基本操作

### 1.1 创建一个索引

```json
#创建一个索引
PUT /person
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1
  }
}
```

### 1.2 查看索引信息

```json
#查看索引
GET /person
```

### 1.3 删除索引

```json
#删除索引
DELETE /person
```



### 1.4 ES中Field可以指定的类型

~~~json
#String:
	text：一般用于全文检索。将当前的field进行分词
# keyword: 当前的Field不可被分词 
~~~



### 1.5 创建索引并指定数据结构

——以创建小说为例子

~~~json
PUT /book
{
  "settings": {
      #备份数
    "number_of_replicas": 1,
      #分片数
   	"number_of_shards": 5
  },
    #指定数据结构
  "mappings": {
    #指定类型 Type
    "novel": {
    # 文件存储的Field属性名
      "properties": {
        "name": {
          "type": "text",
          "analyzer": "ik_max_word",
    #   指定当前的Field可以作为查询的条件
          "index": true
        },
        "authoor": {
          "type": "keyword"
        },
        "onsale": {
          "type": "date",
          "format": "yyyy-MM-dd"
        }
      }
    }
  }
}
~~~

### 1.6 文档的操作

- <u>文档在ES服务中的唯一标志，_index,   _type,    _id 三个内容为组合，来锁定一个文档，操作抑或是修改</u>

#### 1.6.1 新建文档

- 自动生成id

~~~Json
PUT /book/novel
{
  "name": "西游记",
  "authoor": "刘明",
  "onsale": "2020-12-11"
}
~~~

- **手动指定ID（更推荐）**

~~~json
PUT /book/novel/1
{
  "name": "三国演义",
  "authoor": "小明",
  "onsale": "2020-12-11"
}

~~~

#### 1.6.2 修改文档 

- <u>覆盖式修改</u>

  ~~~json
  POST /book/novel/1
  {
    "name": "三国演义",
    "authoor": "小明",
    "onsale": "2020-12-11"
  }
  
  
  ~~~

  

- <u>doc修改方式（更推荐）</u>

  ~~~json
  POST /book/novel/1/_update
  {
    "doc": {
      "name": "极品家丁"
    }
  }
  #先锁定文档，_update  修改需要的字段即可
  ~~~

#### 1.6.3  删除文档

- <u>删库跑路</u>

  ~~~json
  DELETE /book/novel/1
  ~~~

  

## 2. java操作ElaticSearch

### 2.1 Java链接ES

~~~Xml
1、创建Maven工程
	导入依赖
#  4个依赖
   1、1  elasticsearch
<!-- https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch -->
<dependency>
    <groupId>org.elasticsearch</groupId>
    <artifactId>elasticsearch</artifactId>
    <version>6.5.4</version>
</dependency>

   1、2  elasticsearch的高级API
<!-- https://mvnrepository.com/artifact/org.elasticsearch.client/elasticsearch-rest-high-level-client -->
<dependency>
    <groupId>org.elasticsearch.client</groupId>
    <artifactId>elasticsearch-rest-high-level-client</artifactId>
    <version>6.5.4</version>
</dependency>

   1、3   junit
<!-- https://mvnrepository.com/artifact/junit/junit -->
<dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>4.12</version>
    <scope>test</scope>
</dependency>

   1、4  lombok
<!-- https://mvnrepository.com/artifact/org.projectlombok/lombok -->
<dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <version>1.18.12</version>
    <scope>provided</scope>
</dependency>
~~~

### 2.2 修改文档

- <u>添加文档操作</u>

  ~~~java
  @Test
      public void createDoc() throws IOException {
          ObjectMapper mapper = new ObjectMapper();
  
  //        1. 准备json数据
          Person person = new Person(1, "张三", 23, new Date());
          String json = mapper.writeValueAsString(person);
          System.out.println(json);
  
  //        2. 准备一个request对象(手动指定id创建)
          IndexRequest indexRequest = new IndexRequest(index,type,person.getId().toString());
          indexRequest.source(json, XContentType.JSON);
  
  //            3、通过client对象执行添加操作
          RestHighLevelClient client = ESClient.getClient();
          IndexResponse resp = client.index(indexRequest, RequestOptions.DEFAULT);
  
  //            4、 输出返回
          System.out.println(resp.getResult().toString());
      }
  ~~~

  

- <u>修改文档</u>

  ~~~java
  //    修改文档，通过doc方式
      @Test
      public void updateDoc() throws IOException {
  //        创建map,指定需要修改的内容
          Map<String,Object> map = new HashMap<String, Object>();
          map.put("name","李四");
          String docId = "1";
  //           创建一个request对象，封装数据
          UpdateRequest updateRequest = new UpdateRequest(index,type,docId);
          updateRequest.doc(map);
  //        通过client对象执行
          RestHighLevelClient client = ESClient.getClient();
          UpdateResponse update = client.update(updateRequest, RequestOptions.DEFAULT);
  //                返回输出结果
          System.out.println(update.getResult().toString());
      }
  ~~~

### 2.3 Java ES使用注意事项

1、修改字段类型或者修改分词器都要删除索引，然后重新构建

## 3. ES的各种查询

### 3.1 term&terms查询

#### 3.1.1 term查询

- 	<u>term的查询是代表完全匹配，搜索之前不会对你的关键字进行分词</u>

```json
#term匹配查询
POST /sms_logs_index/sms_logs_type/_search
{
  "from": 0,   #limit  from,size
  "size": 5,
  "query": {
    "term": {
      "province": {
        "value": "河北"
      }
    }
  }
}
##不会对term中所匹配的值进行分词查询
```



```java
// java代码实现方式
    @Test
    public void testQuery() throws IOException {
//        1 创建Request对象
        SearchRequest request = new SearchRequest(index);
        request.types(type);
//        2 指定查询条件
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.from(0);
        builder.size(5);
        builder.query(QueryBuilders.termQuery("province", "河北"));

        request.source(builder);
//        3 执行查询
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
//        4  获取到_source中的数据
        for (SearchHit hit : response.getHits().getHits()) {
            Map<String, Object> result = hit.getSourceAsMap();
            System.out.println(result);
        }
    }
```



- <u>terms是针对一个字段包含多个值得运用</u>

  - <u>terms: where province = 河北 or province = ? or province = ?</u>

  ```json
  #terms 匹配查询
  POST /sms_logs_index/sms_logs_type/_search
  {
    "from": 0,
    "size": 5,
    "query": {
      "terms": {
        "province": [
          "河北",
          "河南"
        ]
      }
    }
  }
  ```

  ```java
  // java代码 terms 查询
   @Test
      public void test_terms() throws IOException {
          SearchRequest request = new SearchRequest(index);
          request.types(type);
  
          SearchSourceBuilder builder = new SearchSourceBuilder();
          builder.query(QueryBuilders.termsQuery("province","河北","河南"));
  
          request.source(builder);
  
          RestHighLevelClient client = ESClient.getClient();
          SearchResponse resp = client.search(request, RequestOptions.DEFAULT);
  
          for (SearchHit hit : resp.getHits().getHits()){
              System.out.println(hit);
          }
      }
  ```



### 3.2 match查询

<u>match查询属于高层查询，它会根据你查询字段类型不一样，采用不同的查询方式</u>

<u>match查询，实际底层就是多个term查询，将多个term查询的结果进行了封装</u>

- <u>查询的如果是日期或者是数值的话，它会根据你的字符串查询内容转换为日期或者是数值对等</u>

- <u>如果查询的内容是一个不可被分的内容（keyword），match查询不会对你的查询的关键字进行分词</u>

- <u>如果查询的内容是一个可被分的内容（text）,match则会根据指定的查询内容按照一定的分词规则去分词进行查询</u>

  

#### 3.2.1 match_all查询

<u>查询全部内容，不指定任何查询条件</u>

~~~json
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "match_all": {}
  }
}
~~~

~~~java
 @Test
    public void test_match_all() throws IOException {
      // 创建Request  ,放入索引和类型
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        builder.size(20); //es默认查询结果只展示10条，这里可以指定展示的条数
        //指定查询条件
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.matchAllQuery());
        request.source(builder);
        // 执行查询
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
		// 获取查询结果，遍历显示
        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit);
        }
    }
~~~

#### 3.2.2 match查询 根据某个Field

~~~json
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "match": {
      "smsContent": "打车"
    }
  }
}
~~~

~~~java
 @Test
    public void test_match_field() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.matchQuery("smsContext","打车"));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);

        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit);
        }

    }
~~~



### 3.3 Or或者And

<u>基于一个Filed匹配的内容，采用and或者or的方式进行连接</u>

~~~json
# 布尔match查询
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "match": {
      "smsContext": {
        "query": "打车 女士",
        "operator": "and"   #or
      }
    }
  }
}
~~~

~~~java
@Test
    public void test_match_boolean() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.matchQuery("smsContext","打车 女士").operator(Operator.AND));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);

        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit);
        }
~~~

#### 



### 3.4- match、 match_prrase 和match_phrase_prefix的区别

- match 按照条件查询
- match_phrase 短句查询
- Match_phrase_prefix 最左前缀查询

```java
PUT t1/doc/1
{
  "title": "中国是世界上人口最多的国家"
}
PUT t1/doc/2
{
  "title": "美国是世界上军事实力最强大的国家"
}
PUT t1/doc/3
{
  "title": "北京是中国的首都"
}
```

当我们以`中国`作为搜索条件，我们希望只返回和`中国`相关的文档。我们首先来使用`match`查询：

```java
{
  "query": {
    "match": {
      "title": "中国"
    }
  }
}
// 虽然如期的返回了中国的文档。但是却把和美国的文档也返回了，这并不是我们想要的。是怎么回事呢？
//因为这是elasticsearch在内部对文档做分词的时候，对于中文来说，就是一个字一个字分的
//所以，我们搜中国，中和国都符合条件，返回，而美国的国也符合。
```

而我们认为`中国`是个短语，是一个有具体含义的词。所以elasticsearch在处理中文分词方面比较弱势。后面会讲针对中文的插件。
但目前我们还有办法解决，那就是使用短语查询：

```java
GET t1/doc/_search
{
  "query": {
    "match_phrase": {
      "title": {
        "query": "中国"
      }
    }
  }
}
//这里match_phrase是在文档中搜索指定的词组，而中国则正是一个词组，所以愉快的返回了。
```

那么，现在我们要想搜索`中国`和`世界`相关的文档，但又忘记其余部分了，怎么做呢？用`match`也不行，那就继续用`match_phrase`试试：

```java
GET t1/doc/_search
{
  "query": {
    "match_phrase": {
      "title": "中国世界"
    }
  }
}
//返回结果也是空的，因为没有中国世界这个短语。
//我们搜索中国和世界这两个指定词组时，但又不清楚两个词组之间有多少别的词间隔。
//那么在搜的时候就要留有一些余地。这时就要用到了slop了。相当于正则中的中国.*?世界。
//这个间隔默认为0，导致我们刚才没有搜到,现在我们指定一个间隔
```

```java
GET t1/doc/_search
{
  "query": {
    "match_phrase": {
      "title": {
        "query": "中国世界",
        "slop": 2
      }
    }
  }
}
//现在，两个词组之间有了2个词的间隔，这个时候，就可以查询到结果了
```

`slop`间隔你可以根据需要适当改动。

现在凌晨2点半，单身狗小黑为了缓解寂寞，就准备搜索几个`beautiful girl`来陪伴自己。但是由于英语没过2级，但单词`beautiful`拼到`bea`就不知道往下怎么拼了。这个时候，我们的智能搜索要帮他啊，elasticsearch就看自己的词库有啥事`bea`开头的词，结果还真发现了两个：

```java
PUT t3/doc/1
{
  "title": "maggie",
  "desc": "beautiful girl you are beautiful so"
}
PUT t3/doc/2
{
  "title": "sun and beach",
  "desc": "I like basking on the beach"
}
```

但这里用`match`和`match_phrase`都不太合适，因为小黑输入的不是完整的词。那怎么办呢？我们用`match_phrase_prefix`来搞：

```java
GET t3/doc/_search
{
  "query": {
    "match_phrase_prefix": {
      "desc": "bea"
    }
  }
}
```

前缀查询是短语查询类似，但前缀查询可以更进一步的搜索词组，只不过它是和词组中最后一个词条进行前缀匹配（如搜这样的`you are bea`）。应用也非常的广泛，比如搜索框的提示信息，当使用这种行为进行搜索时，最好通过`max_expansions`来设置最大的前缀扩展数量，因为产生的结果会是一个很大的集合，不加限制的话，影响查询性能。

#### 3.4.1最左查询不到，用or分词

比如你的内容是：《资源的再生问题》，你用**match_phrase_prefix**搜索《资源》的时候是可以搜索到以资源开头的问题的，但是如果你用**match_phrase_prefix**搜索的是《资源问题》，那么你将搜索不到《资源的再生问题》，因为match_phrase_prefix只做前缀匹配，这个时候可以用should进行2个都查询 

#### 3.4.2 multi_match查询

<u>match针对一个field做检索，multi_match针对多个field进行检索，多个key对应一个text</u>

~~~json
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "multi_match": {
      "query": "河北",  #指定text
      "fields": ["province","smsContext"] #指定field
    }
  }
}
~~~

~~~java
// java 实现 
@Test
    public void test_multi_match() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        // 查询的文本内容  字段1 字段2 字段3 。。。。。
        builder.query(QueryBuilders.multiMatchQuery("河北", "province", "smsContext"));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        for (SearchHit hit : response.getHits().getHits()) {
            System.out.println(hit);
        }
    }
~~~

### 3.5term 和match的区别

term查询的是没有经过分析的查询关键字。但是，这同样需要限制，如果你要查询的字段类型（`类型是`text`）是`text`（因为elasticsearch会对文档进行分析，上面说过），那么你得到的可能是不尽如人意的结果或者压根没有结果：

所以，我们这里得到一个论证结果：**不要使用term对类型是text的字段进行查询**，要查询text类型的字段，请改用match查询。

那，term查询可以查询哪些类型的字段呢，例如elasticsearch会将keyword类型的字段当成一个token保存到倒排索引上，你可以将term和keyword结合使用。

最后，要想使用term查询一个字段的多个精确的值怎么办？用terms

所以term适用用Integer Long 或者查询一个精确的字段text或者keyword类型均可

terms可以做组合查询，参考：[terms ](https://www.jianshu.com/p/d5583dff4157)

而match适合text分词查询

### 3.6 ES 的其他查询

#### 3.6.1  ID 查询

~~~JSON
# id查询
GET /sms_logs_index/sms_logs_type/1
GET /索引名/type类型/id
~~~



~~~java
public void test_multi_match() throws IOException {
        GetRequest request = new GetRequest(index,type,"1");
        RestHighLevelClient client = ESClient.getClient();
        GetResponse resp = client.get(request, RequestOptions.DEFAULT);
        System.out.println(resp.getSourceAsMap());
    }
~~~

#### 3.6.2 ids查询

<u>根据多个id进行查询，类似MySql中的where Id in (id1,id2,id3….)</u>

~~~json
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "ids": {
      "values": [1,2,3]  #id值
    }
  }
}
~~~

~~~java
  //java代码

	@Test
    public void test_query_ids() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.idsQuery().addIds("1","2","3"));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit.getSourceAsMap());
        }

    }
~~~

#### 3.6.3prefix查询

<u>前缀查询，可以通过一个关键字去指定一个Field的前缀，从而查询到指定的文档</u>

~~~json
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "prefix": {
      "smsContext": {
        "value": "河"
      }
    }
  }
}
#与 match查询的不同在于，prefix类似mysql中的模糊查询。而match的查询类似于严格匹配查询
 # 针对不可分割词
~~~

~~~java
 @Test
    public void test_query_prefix() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.prefixQuery("smsContext","河"));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit.getSourceAsMap());
        }
    }
~~~

#### 3.6.4fuzzy查询

<u>fuzzy查询：模糊查询，我们可以输入一个字符的大概，ES就可以根据输入的内容大概去匹配一下结果，eg.你可以存在一些错别字</u>

~~~json
#fuzzy查询
#fuzzy查询
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "fuzzy": {
      "corpName": {
        "value": "盒马生鲜",
        "prefix_length": 2  # 指定前几个字符要严格匹配
      }
    }
  }
}

#不稳定，查询字段差太多也可能查不到
~~~

~~~java
// java 实现
    @Test
    public void test_query_fuzzy() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.fuzzyQuery("corpName","盒马生鲜").prefixLength(2));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit.getSourceAsMap());
        }
    }
 .prefixLength() :指定前几个字符严格匹配
~~~

#### 3.6.5 wildcard查询

 <u>通配查询，与mysql中的like查询是一样的，可以在查询时，在字符串中指定通配符*和占位符？</u>

~~~json
#wildcard查询
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "wildcard": {
      "corpName": {
        "value": "*车"   # 可以使用*和？指定通配符和占位符
      }
    }
  }
}
?代表一个占位符
??代表两个占位符
~~~

~~~java
// java代码
@Test
    public void test_query_wildcard() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
       builder.query(QueryBuilders.wildcardQuery("corpName","*车"));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit.getSourceAsMap());
        }
    }
~~~

#### 3.6.6range查询

<u>范围查询，只针对数值类型，对某一个Field进行大于或者小于的范围指定</u>

 ~~~json
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "range": {
      "relyTotal": {
        "gte": 0,  
        "lte": 3
      }
    }
  }
}

查询范围:[gte,lte]
查询范围：(gt,lt)
 ~~~

~~~java
//java代码
@Test
    public void test_query_range() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.rangeQuery("fee").lt(5).gt(2));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit.getSourceAsMap());
        }
    }
~~~

#### 3.6.7regexp查询

<u>正则查询，通过你编写的正则表达式去匹配内容</u>

<u>PS: prefix,fuzzy,wildcar和regexp查询效率相对比较低,在对效率要求比较高时，避免去使用</u>

~~~json
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "regexp": {
      "moible": "109[0-8]{7}"  # 匹配的正则规则
    }
  }
}
~~~

~~~java
//java 代码
   @Test
    public void test_query_regexp() throws IOException {
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.regexpQuery("moible","106[0-9]{8}"));
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        for (SearchHit hit : response.getHits().getHits()){
            System.out.println(hit.getSourceAsMap());
        }
    }
~~~

### 3.8 深分页Scroll

<u>ES对from+size有限制，from和size两者之和不能超过1w</u>

<u>原理：</u>

~~~html
from+size  ES查询数据的方式：
 	1  先将用户指定的关键词进行分词处理
    2  将分词去词库中进行检索，得到多个文档的id
    3  去各个分片中拉去指定的数据   耗时
    4  根据数据的得分进行排序       耗时
    5  根据from的值，将查询到的数据舍弃一部分，
    6  返回查询结果

Scroll+size    在ES中查询方式
	1  先将用户指定的关键词进行分词处理
    2  将分词去词库中进行检索，得到多个文档的id
    3  将文档的id存放在一个ES的上下文中，ES内存
	4  根据你指定给的size的个数去ES中检索指定个数的数据，拿完数据的文档id,会从上下文中移除
    5  如果需要下一页的数据，直接去ES的上下文中，找后续内容
	6  循环进行4.5操作
~~~

<u>缺点，Scroll是从内存中去拿去数据的，不适合做实时的查询，拿到的数据不是最新的</u>

~~~json
# 执行scroll查询，返回第一页数据，并且将文档id信息存放在ES的上下文中，指定生存时间
POST /sms_logs_index/sms_logs_type/_search?scroll=1m
{
  "query": {
    "match_all": {}
  },
  "size": 2,
  "sort": [
    {
      "fee": {
        "order": "desc"
      }
    }
  ]
}
#查询下一页的数据
POST /_search/scroll
{
  "scroll_id": "DnF1ZXJ5VGhlbkZldGNoAwAAAAAAACSPFnJjV1pHbENVVGZHMmlQbHVZX1JGdmcAAAAAAAAkkBZyY1daR2xDVVRmRzJpUGx1WV9SRnZnAAAAAAAAJJEWcmNXWkdsQ1VUZkcyaVBsdVlfUkZ2Zw==",
  "scoll" :"1m"  #scorll信息的生存时间
}
#删除scroll在ES中上下文的数据
DELETE /_search/scroll/scrill_id
~~~



~~~java
//java代码
  @Test
    public void test_query_scroll() throws IOException {
//        1   创建SearchRequest
        SearchRequest request = new SearchRequest(index);
        request.types(type);
//        2   指定scroll信息,生存时间
        request.scroll(TimeValue.timeValueMinutes(1L));
//        3   指定查询条件
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.size(2);
        builder.sort("fee",SortOrder.DESC);
        builder.query(QueryBuilders.matchAllQuery());
//        4 获取返回结果scrollid ,source
        request.source(builder);
        RestHighLevelClient client = ESClient.getClient();
        SearchResponse response = client.search(request,RequestOptions.DEFAULT);
        String scrollId = response.getScrollId();
        System.out.println(scrollId);
        while(true){
//       5  循环创建SearchScrollRequest
        SearchScrollRequest scrollRequest = new SearchScrollRequest(scrollId);
        // 6 指定scrollid生存时间
        scrollRequest.scroll(TimeValue.timeValueMinutes(1L));
//        7 执行查询获取返回结果
        SearchResponse scrollResp = client.scroll(scrollRequest, RequestOptions.DEFAULT);
//        8.判断是否得到数据，输出
           if (scrollResp.getHits().getHits() != null && scrollResp.getHits().getHits().length > 0){
               System.out.println("=======下一页的数据========");
               for (SearchHit hit : scrollResp.getHits().getHits()){
                   System.out.println(hit.getSourceAsMap());
               }
           }else{
               //        9。判断没有查询到数据-退出循环
               System.out.println("没得");
               break;
           }
        }
        // 10  创建clearScrollRequest
        ClearScrollRequest clearScrollRequest = new ClearScrollRequest();
        // 11 指定scrollid
        clearScrollRequest.addScrollId(scrollId);
        // 12  删除
        client.clearScroll(clearScrollRequest,RequestOptions.DEFAULT);
    }
~~~

###  3.9delete-by-query

<u>根据term，match等查询方式去删除大量的文档</u>

<u>如果你需要删除的内容，是index下的大部分数据，不建议使用，建议逆向操作，创建新的索引，添加需要保留的数据内容</u>

~~~json
POST /sms_logs_index/sms_logs_type/_delete_by_query
{
  "query": {
    "range": {
      "relyTotal": {
        "gte": 2,
        "lte": 3
      }
    }
  }
}

##中间跟你的查询条件，查到什么，删什么t
~~~

~~~java
public class test_sms_search2 {
    String index = "sms_logs_index";
    String type = "sms_logs_type";
    @Test
    public void test_query_fuzzy() throws IOException {
        DeleteByQueryRequest request = new DeleteByQueryRequest(index);
        request.types(type);

        request.setQuery(QueryBuilders.rangeQuery("relyTotal").gt("2").lt("3"));

        RestHighLevelClient client = ESClient.getClient();
        BulkByScrollResponse response = client.deleteByQuery(request, RequestOptions.DEFAULT);

        System.out.println(response.toString());
    }
}
~~~

### 4 复合查询

### 4. 1 bool查询

<u>复合过滤器，可以将多个查询条件以一定的逻辑组合在一起，and  or</u>

- must : <u>所有的条件，用must组合在一起，表示AND</u>

-  must_not:<u>将must_not中的条件，全部不能匹配，表示not的意思，不能匹配该查询条件</u>
- should: <u>所有条件，用should组合在一起，表示or的意思，文档必须匹配一个或者多个查询条件</u>
- filter: <u>过滤器，文档必须匹配该过滤条件，跟must子句的唯一区别是，filter不影响查询的score</u>

~~~json
#查询省份为河北或者河南的
#并且公司名不是河马生鲜的
#并且smsContext中包含软件两个字
POST /sms_logs_index/sms_logs_type/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "term": {
            "province": {
              "value": "河北"
            }
          }
        },
        {
          "term": {
            "province": {
              "value": "河南"
            }
           
        }
      ],
    "must_not": [
      {
        "term": {
          "corpName": {
            "value": "河马生鲜"
          }
        }
      }
    ],
     "must": [
       {
         "match": {
           "smsContext": "软件"
         }
       }
     ]
    }
  }
}

~~~

#####  

```java
public void  boolSearch() throws IOException {

        //  1.创建 searchRequest
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        // 2.指定查询条件
        SearchSourceBuilder builder = new SearchSourceBuilder();
        BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder();
        // #省是 晋城 或者北京
        boolQueryBuilder.should(QueryBuilders.termQuery("province","北京"));
        boolQueryBuilder.should(QueryBuilders.termQuery("province","晋城"));

        //# 运营商不能是联通
        boolQueryBuilder.mustNot(QueryBuilders.termQuery("operatorId",2));

        //#smsContent 包含 战士 和的
        boolQueryBuilder.must(QueryBuilders.matchQuery("smsContent","战士"));
        boolQueryBuilder.must(QueryBuilders.matchQuery("smsContent","的"));

        builder.query(boolQueryBuilder);
        request.source(builder);
        //  3.执行查询
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        // 4.输出结果
        for (SearchHit hit : response.getHits().getHits()) {
            System.out.println(hit.getSourceAsMap());
        }
    }
```

#### 4.4.1  boosting 查询

```
boosting 查询可以帮助我们去影响查询后的score
   positive:只有匹配上positive 查询的内容，才会被放到返回的结果集中
   negative: 如果匹配上了positive 也匹配上了negative, 就可以 降低这样的文档score.
   negative_boost:指定系数,必须小于1   0.5 
关于查询时，分数时如何计算的：
	搜索的关键字再文档中出现的频次越高，分数越高
	指定的文档内容越短，分数越高。
	我们再搜索时，指定的关键字也会被分词，这个被分词的内容，被分词库匹配的个数越多，分数就越高。
   
```

```json
#boosting 查询
POST /sms-logs-index/sms-logs-type/_search
{
  "query": {
    "boosting": {
      "positive": {
        "match": {
          "smsContent": "战士"
        }
      }, 
      "negative": {
        "match": {
          "smsContent": "团队"
        }
      },
      "negative_boost": 0.2
    }
  }
}
```

```java
    public void  boostSearch() throws IOException {

        //  1.创建 searchRequest
        SearchRequest request = new SearchRequest(index);
        request.types(type);
        // 2.指定查询条件
        SearchSourceBuilder builder = new SearchSourceBuilder();
        BoostingQueryBuilder boost = QueryBuilders.boostingQuery(
                QueryBuilders.matchQuery("smsContent", "战士"),
                QueryBuilders.matchQuery("smsContent", "团队")
        ).negativeBoost(0.2f);
        builder.query(boost);
        request.source(builder);
        //  3.执行查询
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        // 4.输出结果
        for (SearchHit hit : response.getHits().getHits()) {
            System.out.println(hit.getSourceAsMap());
        }
    }
```

#### 4.1.2 filter  查询

query 查询：根据你的查询条件，去计算文档的匹配度得到一个分数，并根据分数排序，不会做缓存的。

filter 查询：根据查询条件去查询文档，不去计算分数，而且filter会对经常被过滤的数据进行缓存。

```json
#filter 查询
POST /sms-logs-index/sms-logs-type/_search
{
  "query": {
    "bool": {
      "filter": [
        {
          "term": {
            "corpName": "海尔智家公司"
           }
        },
        {
          "range":{
            "fee":{
              "lte":50
            }
          }
        }
      ]
    }
  }
}
```

```java
    public void filter() throws IOException {

        // 1.searchRequest
        SearchRequest searchRequest = new SearchRequest(index);
        searchRequest.types(type);

        // 2.指定查询条件
        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
        BoolQueryBuilder boolBuilder = QueryBuilders.boolQuery();
        boolBuilder.filter(QueryBuilders.termQuery("corpName","海尔智家公司"));
        boolBuilder.filter(QueryBuilders.rangeQuery("fee").gt(20));
        sourceBuilder.query(boolBuilder);
        searchRequest.source(sourceBuilder);

        //  3.执行
        SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);
        
        //  4. 输出结果
        for (SearchHit hit : response.getHits().getHits()) {
            System.out.println(hit.getSourceAsMap());
            System.out.println(hit.getId()+"的分数是："+hit.getScore());
        }
    }
```

#### 4.1.3 高亮查询

```
高亮查询就是用户输入的关键字，以一定特殊样式展示给用户，让用户知道为什么这个结果被检索出来
高亮展示的数据，本身就是文档中的一个field,单独将field以highlight的形式返回给用户
ES提供了一个highlight 属性，他和query 同级别。
 frament_size: 指定高亮数据展示多少个字符回来
 pre_tags:指定前缀标签<front color="red">
 post_tags:指定后缀标签 </font>
 
```

```json
#highlight 高亮查询
POST /sms-logs-index/sms-logs-type/_search
{
  "query": {
    "match": {
      "smsContent": "团队"
    }
  },
  "highlight": {
    "fields": {
      "smsContent":{}
    },
    "pre_tags":"<font color='red'>",
    "post_tags":"</font>",
    "fragment_size":10
  }
}
```

```java
 public void highLightQuery() throws IOException {
      // 1.创建request
        SearchRequest request = new SearchRequest(index);
        request.types(type);

      // 2.指定查询条件，指定高亮
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.query(QueryBuilders.matchQuery("smsContent","团队"));
        HighlightBuilder highlightBuilder = new HighlightBuilder();
        highlightBuilder.field("smsContent",10)
                .preTags("<font colr='red'>")
                .postTags("</font>");
        builder.highlighter(highlightBuilder);
        request.source(builder);

      // 3.执行
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);

        //4. 输出结果
        for (SearchHit hit : response.getHits().getHits()) {
            System.out.println(hit.getHighlightFields().get("smsContent"));
        }
    }
```

#### 4.1.4 聚合查询

```
ES的聚合查询和mysql 的聚合查询类似，ES的聚合查询相比mysql 要强大得多。ES提供的统计数据的方式多种多样。

```

```json
#ES 聚合查询的RSTFul 语法
POST /index/type/_search
{
    "aggs":{
        "(名字)agg":{
            "agg_type":{
                "属性"："值"
            }
        }
    }
}
```

#### 4.1.5 去重计数聚合查询

```
去重计数，cardinality 先将返回的文档中的一个指定的field进行去重，统计一共有多少条
```

```json
# 去重计数 查询 province
POST /sms-logs-index/sms-logs-type/_search
{
  "aggs": {
    "provinceAgg": {
      "cardinality": {
        "field": "province"
      }
    }
  }
}
```

```java
    public void aggCardinalityC() throws IOException {

        // 1.创建request
        SearchRequest request = new SearchRequest(index);
        request.types(type);

        // 2. 指定使用聚合查询方式
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.aggregation(AggregationBuilders.cardinality("provinceAgg").field("province"));
        request.source(builder);

        // 3.执行查询
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);

        // 4.输出返回结果
        Cardinality agg = response.getAggregations().get("provinceAgg");
        System.out.println(agg.getValue());
    }
```

#### 4.1.6 范围统计

```
统计一定范围内出现的文档个数，比如，针对某一个field 的值再0~100,100~200,200~300 之间文档出现的个数分别是多少
范围统计 可以针对 普通的数值，针对时间类型，针对ip类型都可以响应。
数值 rang    
时间  date_rang     
ip   ip_rang
```

```json
#针对数值方式的范围统计  from 带等于效果 ，to 不带等于效果
POST /sms-logs-index/sms-logs-type/_search
{
  "aggs": {
    "agg": {
      "range": {
        "field": "fee",
        "ranges": [
          {
            "to": 30
          },
           {
            "from": 30,
            "to": 60
          },
          {
            "from": 60
          }
        ]
      }
    }
  }
}
#时间方式统计
POST /sms-logs-index/sms-logs-type/_search
{
  "aggs": {
    "agg": {
      "date_range": {
        "field": "sendDate",
        "format": "yyyy", 
        "ranges": [
          {
            "to": "2000"
          },{
            "from": "2000"
          }
        ]
      }
    }
  }
}
#ip 方式 范围统计
POST /sms-logs-index/sms-logs-type/_search
{
  "aggs": {
    "agg": {
      "ip_range": {
        "field": "ipAddr",
        "ranges": [
          {
            "to": "127.0.0.8"
          },
          {
            "from": "127.0.0.8"
          }
        ]
      }
    }
  }
}
```

```java
 public void aggRang() throws IOException {
        // 1.创建request
        SearchRequest request = new SearchRequest(index);
        request.types(type);

        // 2. 指定使用聚合查询方式
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.aggregation(AggregationBuilders.range("agg").field("fee")
                            .addUnboundedTo(30)
                            .addRange(30,60)
                            .addUnboundedFrom(60));
        request.source(builder);

        // 3.执行查询
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);

        // 4.输出返回结果
        Range agg = response.getAggregations().get("agg");
        for (Range.Bucket bucket : agg.getBuckets()) {
            String key = bucket.getKeyAsString();
            Object from = bucket.getFrom();
            Object to = bucket.getTo();
            long docCount = bucket.getDocCount();
            System.out.println(String.format("key: %s ,from: %s ,to: %s ,docCount: %s",key,from,to,docCount));
        }
    }
```

#### 4.1.7 统计聚合

```
他可以帮你查询指定field 的最大值，最小值，平均值，平方和...
使用 extended_stats
```

```json
#统计聚合查询 extended_stats
POST /sms-logs-index/sms-logs-type/_search
{
  "aggs": {
    "agg": {
      "extended_stats": {
        "field": "fee"
      }
    }
  }
}
```

```java
// java实现   
public void aggExtendedStats() throws IOException {
        // 1.创建request
        SearchRequest request = new SearchRequest(index);
        request.types(type);

        // 2. 指定使用聚合查询方式
        SearchSourceBuilder builder = new SearchSourceBuilder();
        builder.aggregation(AggregationBuilders.extendedStats("agg").field("fee"));
        request.source(builder);

        // 3.执行查询
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);

        // 4.输出返回结果
       ExtendedStats extendedStats =  response.getAggregations().get("agg");
        System.out.println("最大值："+extendedStats.getMaxAsString()+",最小值："+extendedStats.getMinAsString());
    }
```

#### 4.1.8  其他聚合查询 查看官方文档

https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-aggregations-metrics-weight-avg-aggregation.html

 



### 4.2纠错和模糊查询

#### fuzzy模糊查询

`fuzziness`的默认值是2 —— 表示最多可以纠错两次.

说明: `fuzziness`的值太大, 将削弱检索条件的作用, 也就是说纠错次数太多, 就会导致限定检索结果的检索条件被改变, 失去了限定作用.





#### term纠错一个词

term纠错:返回的结果是单个词，即使你输入的是一个句子，返回的也是一个词，对中文的支持差

```java
POST /xiyou_student_index/_search
{
  "suggest": {
    "my-suggestion": {
      "text": " lucne and elasticsear rock hello world",
      "term": {
        "field": "name"
         
      }
    }
  }
}
```

```java
{
  "took": 8,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": 0,
    "hits": []
  },
  "suggest": {
    "my-suggestion": [
      {
        "text": "lucne",
        "offset": 1,
        "length": 5,
        "options": [
          {
            "text": "lucene",
            "score": 0.8,
            "freq": 2
          }
        ]
      },
      {
        "text": "and",
        "offset": 7,
        "length": 3,
        "options": []
      },
      {
        "text": "elasticsear",
        "offset": 11,
        "length": 11,
        "options": [
          {
            "text": "elasticsearch",
            "score": 0.8181818,
            "freq": 2
          }
        ]
      },
      {
        "text": "rock",
        "offset": 23,
        "length": 4,
        "options": [
          {
            "text": "rocks",
            "score": 0.75,
            "freq": 2
          }
        ]
      },
      {
        "text": "hello",
        "offset": 28,
        "length": 5,
        "options": []
      },
      {
        "text": "world",
        "offset": 34,
        "length": 5,
        "options": []
      }
    ]
  }
}
```

#### phrase 纠错一个句子

Phrase 会返回最接近的一个句子，而不仅仅是一个词，当然如果你给你的是只有2-3个词，它也不会返回一个句子，只是返回那两三个词的纠错：比如

```java
rock hello world
会给纠错成：rock hello world或者rocks hello world

```



```java
POST /xiyou_student_index/_search
{
  "suggest": {
    "my-suggestion": {
      "text": "lucne and elasticsear rock hello world",
      "phrase": {
        "field": "name",
        "analyzer":"ik_smart"
         
      }
    }
  }
}
```

 

```java
{
  "took": 7,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": 0,
    "hits": []
  },
  "suggest": {
    "my-suggestion": [
      {
        "text": "lucne and elasticsear rock hello world",
        "offset": 0,
        "length": 38,
        "options": [
          {
            "text": "lucene elasticsearch rock hello world",
            "score": 0.014350784
          },
          {
            "text": "lucene elasticsear rock hello world",
            "score": 0.011332202
          },
          {
            "text": "lucne elasticsearch rock hello world",
            "score": 0.009595718
          },
          {
            "text": "lucne elasticsear rocks hello world",
            "score": 0.009239876
          }
        ]
      }
    ]
  }
}
```

#### completion 前缀提示



```java
这些参数都是在建议的completion对象的下面：
当在运行建议的请求时，可以决定出现哪些建议，就像其他建议器一样，size参数控制返回多少项建议（默认为5项）；还可以通过fuzzy参数设置模糊建议，以对拼写进行容错。当开启模糊建议之后，可以设置下列参数来完成建议：

fuzziness，可以指定所允许的最大编辑距离。
min_length，指定什么长度的输入文本可以开启模糊查询。
prefix_length，假设若干开始的字符是正确的（比如block，如果输入blaw，该字段也认为之前输入的是对的），这样可以通过牺牲灵活性提升性能。
这些参数都是在建议的completion对象的下面：
GET s8/doc/_search
{
  "suggest": {
    "my_s9": {
      "text": "blaw",
      "completion": {
        "field": "title",
        "size": 2,
        "fuzzy": {
          "fuzziness": 2,
          "min_length": 3,
          "prefix_length": 2
        }
      }
    }
  }
}
```

completion只适用于前缀提示，中间的都不行

```java
_source
为了减少不必要的响应，我们可以对建议结果做一些过滤，比如加上_source：

GET s8/doc/_search
{
  "suggest": {
    "completion_suggest": {
      "text": "appl",
      "completion": {
        "field": "title"
      }
    }
  },
  "_source": "title"
}
```

**skip_duplicates**
我们的建议可能是来自不同的文档，这其中就会有一些重复建议项，我们可以通过设置`skip_duplicates:true`来修改此行为，如果为`true`则会过滤掉结果中的重复建议文档：

```java

		 
	GET s8/doc/_search
{
  "suggest": {
    "completion_suggest": {
      "prefix": "app",
      "completion": {
        "field": "title",
        "size": 5,
        "skip_duplicates":true
      }
    }
  },
  "_source": "title"
}
```

```java
但需要注意的是，该参数设置为true的话，可能会降低搜索速度，因为需要访问更多的建议结果项，才能过滤出来前N个。
最后，完成建议器还支持正则表达式查询，这意味着我们可以将前缀表示为正则表达式：

GET s5/doc/_search
{
  "suggest": {
    "completion_suggest": {
      "regex": "e[l|e]a",
      "completion": {
        "field": "title"
      }
    }
  }
}
```



```java
 GET /_cat/health?v
GET /_cat/nodes?v
GET /_cat/shards
GET _cat/allocation?v  


GET /xiyou_student_index/_mapping
GET /knowledge/_mapping



DELETE xiyou_student_index
 
POST _analyze
{"analyzer": "ik_smart","text": "如何申请微信公众号"}


GET /knowledge/_search
{"query":{"bool":{"must":[{"match":{"appId":{"query":45,"operator":"OR","prefix_length":0,"max_expansions":50,"fuzzy_transpositions":true,"lenient":false,"zero_terms_query":"NONE","auto_generate_synonyms_phrase_query":true,"boost":1.0}}},{"multi_match":{"query":"两个群  hi好好考哈哈卡的","fields":["keyword^1.0","similarQuestions^0.5","standardQuestion^1.5"],"type":"best_fields","operator":"OR","slop":0,"prefix_length":0,"max_expansions":50,"zero_terms_query":"NONE","auto_generate_synonyms_phrase_query":true,"fuzzy_transpositions":true,"boost":1.0}}],"adjust_pure_negative":true,"boost":1.0}},"highlight":{"pre_tags":["<font color='#FF7043'>"],"post_tags":["</font>"],"fields":{"standardQuestion":{},"keyword":{},"similarQuestions":{}}}}


 



 
 

 
```

```java
PUT knowledge_test
 {
    "mappings": {
      "knowledge_test": {
        "properties": {
          "answer": {
            "type": "text",
            "analyzer": "ik_smart"
          },
          "appId": {
            "type": "long"
          },
          "createTime": {
            "type": "long"
          },
          "id": {
            "type": "keyword"
          },
          "keyword": {
            "type": "completion"
          },
          "oriId": {
            "type": "keyword"
          },
          "similarQuestions": {
            "type": "text"
          },
          "standardQuestion": {
            "type": "completion",
            "analyzer": "ik_smart"
          },
          "updateTime": {
            "type": "long"
          }
        }
      }
    }
  
}
```

### 4.3 ES 排序sort

 

 

```java
QueryBuilder q1=QueryBuilders.multiMatchQuery(searchVO.getQ()).fields(boostMap)
        .operator(Operator.OR).type(type);
QueryBuilder q2=QueryBuilders.multiMatchQuery(searchVO.getQ()).fields(boostMap)
        .operator(Operator.OR).type(MultiMatchQueryBuilder.Type.BEST_FIELDS);
BoolQueryBuilder b2 = QueryBuilders.boolQuery();
b2.should(q1);
b2.should(q2);

boolQueryBuilder.must (b2);
```

### 4.4 ES停用词

- [ES-ik热加载停用词和分词](https://blog.csdn.net/zhangxiaohui4445/article/details/103417282)
- [ES-设置停用词和分词](https://www.cnblogs.com/haizhongdenta/p/11950308.html)
- [停用词的优缺点](https://blog.csdn.net/u012549626/article/details/94394893#1.3.%E5%81%9C%E7%94%A8%E8%AF%8D%E5%92%8C%E6%A0%87%E5%87%86%E5%88%86%E6%9E%90%E5%99%A8%EF%BC%88Stopwords%20and%20the%20Standard%20Analyzer%EF%BC%89)

关于一些停用词影响评分的处理

- 问题：用户输入多个《的》会将含有《的》标准问题的评分很高
- 去掉停用词的优缺点：

  - 优点：索引更少的词效率提高、停用词不再影响评分
  - 缺点：英文无法区分 *happy* 和 *not happy*因为英文停用词默认包含but not 等（也可自定义），中文取决于自定义的停用词

解决方案

  - 通过设置IK停用词
    - 优点：简单方便
    - 缺点：每次更新停用词都需要重启ES，线上非常不友好（虽然有一种热加载方式）-不推荐
- 通过代码的方式解决：在搜索的时候提前去掉这些停用词，比如我们只去掉《的、地、吗》
  - 优点：不需要更新IK配置，可以通过VKConf配置停用词
  - 缺点：某些特殊的词去掉后会影响查询，比如：《地上》
    
    - 可以通过先分词再判断（IK分词），比如《地上》分词后还是《地上》，那么《地》就不应该去掉
    

#### ES 匹配查询operator和minimumShouldMatch用法详解

[地址](https://blog.csdn.net/numbbe/article/details/110454270)



### ES 别名

- [ES索引别名博客](https://www.cnblogs.com/Neeo/articles/10897280.html)
- [ES索引别名官网](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html)
- [RestHighLevelClient 别名官网](https://www.elastic.co/guide/en/elasticsearch/client/java-rest/6.4/java-rest-high-update-aliases.html)

#### ES别名的常见应用场景

1. 为了不影响业务100%可用性。

   - 背景：对旧的数据进行了字段或者分词器等调整，需要重构索引
   - 作用：在运行的集群中可以无缝的从一个索引切换到另一个索引
2. 用时间或者其它方式分割方式创建索引，但是查询需要N个小索引聚合的情况
   - 背景：以天为单位创建索引，需要查询某个月的全部日志
   - 作用：将某个月的全部索引都指向一个索引别名，就可以查询某个月的全部日志了

#### ES别名的使用前提

- ES别名和索引名称不能重复
- 平滑迁移索引的时候，新旧索引要同时存在
- **查询必须走别名的方式**
- 如果有多个索引用一个别名，写操作需要指定索引，因为它不知道要往哪个索引写

#### 无抖动切换新索引流程

- 旧索引名称question_old 类型是：question_old_type
- 新索引名称为：question_new 类型是： question_new_type

- 改为用别名查询

  - 1、添加新索引，索引名称是：question_new，type类型是：question_new_type——提前创建

  - 2、给新旧知识库添加别名**question_aliase**——提前创建

  - 3、代码查询直接走别名的形式，下面的indices直接改为**question_aliase**，type还是之前的question_old_type，只不过这个type不是代码里面写死，用配置文件的形式
  - 4、先发布index服务，手动执行新索引的构建
  - 5、等index服务构建好之后，已经有了新索引，再部署query服务，部署期间由于查询的type还是question_old_type所以不会产生抖动，等query服务部署完成后，我们将配置文件的type切换到question_new_type，这个时候便是执行的新索引查询

```java
SearchRequest request=new SearchRequest();
request.indices("qeustion_aliase");
//type采用配置文件的方式
request.types("配置文件的内容");
```
新增配置文件：

```
key :  knowledge.searchType
value: query部署完成前都是knowledge，部署完成后切换成为：knowlege_index
```



#### 无抖动采用别名查询的优点

- 服务部署期间不停机，新旧数据无影响

- 扩展性比较好，未来如果有同样的需求比如：更换分词器或者修改字段类型，同样可以参考上面的形式

#### 注意事项

-   索引别名关系一定要提前创建

- 一定要先部署index模块构建新索引，再部署query模块并切换配置文件

  

  





